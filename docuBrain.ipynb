{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# DocuBrain - Multiple PDF Chat"
      ],
      "metadata": {
        "id": "ax8kRlFDOVQ7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Install Necessary Libraries\n",
        "\n",
        "This cell installs or upgrades the required Python libraries for this project. This includes `langchain` and related packages for building LLM applications, `faiss-cpu` for efficient vector similarity search, `python-dotenv` for managing environment variables, and `pymupdf` for PDF text extraction."
      ],
      "metadata": {
        "id": "x-C07JlLNjLL"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aa6374fe",
        "collapsed": true
      },
      "source": [
        "%pip install --upgrade langchain langchain-community langchain-openai faiss-cpu python-dotenv pymupdf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Import Modules\n",
        "\n",
        "This section imports all the necessary modules from various libraries used throughout the notebook. These include operating system utilities, dotenv for environment variables, `fitz` (PyMuPDF) for robust PDF handling, `CharacterTextSplitter` for breaking text into manageable chunks, `FAISS` for vector storage, `OpenAIEmbeddings` and `ChatOpenAI` for OpenAI's embedding and chat models, `ChatPromptTemplate` for defining chat prompts, `StrOutputParser` for parsing LLM output, and `RunnablePassthrough` for passing inputs directly through a LangChain chain."
      ],
      "metadata": {
        "id": "xLAy-2x5Ns_W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "import fitz\n",
        "from langchain_text_splitters import CharacterTextSplitter\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough"
      ],
      "metadata": {
        "id": "5LnBHgm97zvA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Configure OpenAI API Key\n",
        "\n",
        "This cell handles setting of your OpenAI API key. A placeholder key is provided for demonstration, but you should replace it with actual key or load it securely (e.g., from Colab secrets)."
      ],
      "metadata": {
        "id": "_jfSTu9GOTyb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "load_dotenv()\n",
        "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
        "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY"
      ],
      "metadata": {
        "id": "E1pZxb4Y-88w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. PDF Text Extraction Function\n",
        "\n",
        "### `extract_text_pymupdf(pdf_path)`\n",
        "\n",
        "This function is designed to extract all textual content from a given PDF file. It uses the `fitz` library (PyMuPDF), which is highly efficient for working with PDF documents. The function opens the PDF, iterates through each page, extracts its text, and concatenates it into a single string. This raw text is then returned for further processing."
      ],
      "metadata": {
        "id": "C95iTD40Omj8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_text_pymupdf(pdf_path):\n",
        "    text = \"\"\n",
        "    with fitz.open(pdf_path) as doc:\n",
        "        for page in doc:\n",
        "            text += page.get_text()\n",
        "    return text"
      ],
      "metadata": {
        "id": "50hlGSYi--Kd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Text Chunking Function\n",
        "\n",
        "### `get_text_chunks(text)`\n",
        "\n",
        "This function takes a large string of text and divides it into smaller, overlapping segments called 'chunks'. It employs LangChain's `CharacterTextSplitter`, configured to split text by newline characters, with a `chunk_size` of 1000 characters and an `chunk_overlap` of 200 characters. Text chunking is a critical step for RAG systems, as it prepares documents for efficient embedding and retrieval, ensuring that relevant context can be provided to the LLM without exceeding token limits."
      ],
      "metadata": {
        "id": "uqkhJJ_qQDGk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_text_chunks(text):\n",
        "    text_splitter = CharacterTextSplitter(separator=\"\\n\", chunk_size=1000, chunk_overlap=200, length_function=len)\n",
        "    return text_splitter.split_text(text)"
      ],
      "metadata": {
        "id": "ybWl0XuL_HrY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Batch Embedding Function\n",
        "\n",
        "### `embed_chunks_batched(embeddings, text_chunks, batch_size=10)`\n",
        "\n",
        "This function efficiently generates vector embeddings for a list of text chunks. It utilizes an `embeddings` model (e.g., `OpenAIEmbeddings`) and processes the `text_chunks` in specified `batch_size` groups. This batched approach optimizes API calls and resource usage, accumulating all generated embeddings into a single list."
      ],
      "metadata": {
        "id": "mJjnVWeoQHqC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def embed_chunks_batched(embeddings, text_chunks, batch_size=10):\n",
        "    all_embeddings = []\n",
        "    for i in range(0, len(text_chunks), batch_size):\n",
        "        batch = text_chunks[i:i+batch_size]\n",
        "        batch_embeddings = embeddings.embed_documents(batch)\n",
        "        all_embeddings.extend(batch_embeddings)\n",
        "    return all_embeddings"
      ],
      "metadata": {
        "id": "i6O_5HZz_II1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Vector Store Creation Function\n",
        "\n",
        "### `get_vectorstore(text_chunks, embeddings, embeddings_data=None)`\n",
        "\n",
        "This function is responsible for creating a `FAISS` vector store, which is used for rapid similarity search over the embedded text data. It can operate in two modes: either it takes `text_chunks` and an `embeddings` model to generate embeddings on the fly and build the store, or it uses pre-computed `embeddings_data` directly, combining it with the `text_chunks` to construct the FAISS index. This flexibility allows for optimized vector store creation depending on whether embeddings are already available."
      ],
      "metadata": {
        "id": "wYxCXTcVQLTr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_vectorstore(text_chunks, embeddings, embeddings_data=None):\n",
        "    if embeddings_data:\n",
        "        return FAISS.from_embeddings(list(zip(text_chunks, embeddings_data)), embedding=embeddings)\n",
        "    else:\n",
        "        return FAISS.from_texts(texts=text_chunks, embedding=embeddings)"
      ],
      "metadata": {
        "id": "9oZiVRcA_ILj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Conversational Chain Setup Function\n",
        "\n",
        "### `get_conversation_chain(vectorstore)`\n",
        "\n",
        "This function constructs the conversational retrieval chain, which is the core of the AI's question-answering capability. It initializes a `ChatOpenAI` Large Language Model with a `temperature` of 0 for deterministic responses. It then configures a `retriever` from the provided `vectorstore` to fetch the most relevant documents based on a user's query. A `ChatPromptTemplate` guides the LLM to answer *only* based on the given context, explicitly stating 'I don't know' if the answer isn't found in the provided information. Finally, it builds a LangChain Expression Language (LCEL) chain that integrates the retriever, prompt, LLM, and an output parser to deliver coherent answers."
      ],
      "metadata": {
        "id": "ZzjOFqOjQPK1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_conversation_chain(vectorstore):\n",
        "    llm = ChatOpenAI(temperature=0)\n",
        "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
        "\n",
        "    template = \"\"\"Answer based only on the following context. If the context doesn't contain the answer, say 'I don't know.'\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer:\"\"\"\n",
        "\n",
        "    prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "    def format_docs(docs):\n",
        "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "    chain = (\n",
        "        {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "        | prompt\n",
        "        | llm\n",
        "        | StrOutputParser()\n",
        "    )\n",
        "\n",
        "    return chain"
      ],
      "metadata": {
        "id": "VCecSO6G_IQA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. Main Application Logic\n",
        "\n",
        "### `main()`\n",
        "\n",
        "This is the central function that orchestrates the entire PDF processing and conversational AI workflow. It prompts the user to enter PDF file paths, extracts text from those PDFs, and then divides the text into chunks. It proceeds to generate vector embeddings for these chunks using `OpenAIEmbeddings` and constructs a `FAISS` vector store. Finally, it sets up the conversational chain and enters an interactive loop, allowing the user to ask questions. The AI will provide answers based on the content of the processed PDFs, exiting when the user types 'exit'."
      ],
      "metadata": {
        "id": "ca7PTOfnQSxF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    file_paths = input(\"Enter PDF file paths (comma separated): \").split(\",\")\n",
        "    file_paths = [f.strip() for f in file_paths if f.strip()]\n",
        "    raw_text = \"\\n\".join([extract_text_pymupdf(fp) for fp in file_paths])\n",
        "    text_chunks = get_text_chunks(raw_text)\n",
        "    embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)\n",
        "    embeddings_data = embed_chunks_batched(embeddings, text_chunks)\n",
        "    vectorstore = get_vectorstore(text_chunks, embeddings, embeddings_data)\n",
        "    conversation = get_conversation_chain(vectorstore)\n",
        "    print(\"PDF(s) processed. You can now ask questions. Type 'exit' to quit.\")\n",
        "    while True:\n",
        "        user_question = input(\"Your question: \")\n",
        "        if user_question.strip().lower() == 'exit':\n",
        "            print(\"Goodbye!\")\n",
        "            break\n",
        "        answer = conversation.invoke(user_question)\n",
        "        print(f\"Bot: {answer}\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "KGYvq7ue_IRA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10. Execute the Main Function\n",
        "\n",
        "This cell initiates the `main()` function, starting the DocuBrain conversational AI. The `if __name__ == \"__main__\":` construct ensures that `main()` is called only when the script is executed directly (rather than when imported as a module)."
      ],
      "metadata": {
        "id": "slfbUT0CQdzw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "AUURWpzRQxbm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
